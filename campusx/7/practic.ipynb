{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6743de",
   "metadata": {},
   "outputs": [],
   "source": [
    "API=\"<your openrouter api key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca042b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7e21502",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There isn’t a single definitive answer—different traditions and people give different meanings.  \n",
      "\n",
      "- **Biological:** to survive and reproduce.  \n",
      "- **Philosophical/existential:** to create meaning through choices, values, relationships, and growth.  \n",
      "- **Personal:** whatever gives you a sense of purpose—love, learning, helping others, creating, faith, etc.  \n",
      "\n",
      "In the end, many find that the “meaning of life” is something you shape rather than something you’re given. What feels most meaningful to you right now?\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    extra_headers={\n",
    "        \"HTTP-Referer\": \"<your_site_url>\",\n",
    "        \"X-Title\": \"<your_site_name>\",\n",
    "    },\n",
    "    model=\"openai/gpt-5.2-codex\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the meaning of life?\"\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13da9cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good question. To be precise: I can’t create or send actual image files in this chat. But I *can* help by writing a detailed prompt you can paste into an image generator (like DALL·E, Midjourney, or Stable Diffusion).\n",
      "\n",
      "If you want, tell me the style you prefer (e.g., infographic, futuristic, cartoon), and I’ll craft the prompt for you.\n"
     ]
    }
   ],
   "source": [
    "# First API call with reasoning\n",
    "response = client.chat.completions.create(\n",
    "    model=\"openai/gpt-5.2-codex\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what is LLM generate one image for this\"\n",
    "        }\n",
    "    ],\n",
    "    extra_body={\"reasoning\": {\"enabled\": True}}\n",
    ")\n",
    "\n",
    "# Extract the assistant message with reasoning_details\n",
    "response = response.choices[0].message\n",
    "\n",
    "# Preserve the assistant message with reasoning_details\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"what is LLM generate one image for this\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response.content,\n",
    "        \"reasoning_details\": response.reasoning_details\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Are you sure? Think carefully.\"}\n",
    "]\n",
    "\n",
    "# Second API call\n",
    "response2 = client.chat.completions.create(\n",
    "    model=\"openai/gpt-5.2-codex\",\n",
    "    max_tokens=500,\n",
    "    messages=messages,\n",
    "    extra_body={\"reasoning\": {\"enabled\": True}}\n",
    ")\n",
    "\n",
    "# Extract and display the response\n",
    "response2_text = response2.choices[0].message.content\n",
    "print(response2_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61179abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='gen-1768675409-HaD5NyVakdMwrxJaJ9Wb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Yes — thanks for asking me to double‑check. I don’t have the ability to generate images directly. I can help by creating prompts or descriptions you can use with an image generator.  \\n\\nIf you want, tell me the style you want (realistic, cartoon, futuristic, etc.), and I’ll craft a perfect prompt.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=\"**Clarifying image generation limitation**\\n\\nI'm informing the user that I can't generate images to correct any misunderstanding about LLM capabilities.\", reasoning_details=[{'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.summary', 'summary': \"**Clarifying image generation limitation**\\n\\nI'm informing the user that I can't generate images to correct any misunderstanding about LLM capabilities.\"}, {'id': 'rs_03be69eb3cdc935f01696bd8525e8c819687fa68fb07060e2d', 'format': 'openai-responses-v1', 'index': 0, 'type': 'reasoning.encrypted', 'data': 'gAAAAABpa9hT9eS-QUJ8JlFKGX8foe-7MqNrOI9bt6Gmpb3H-WWjdc2hF3u5ZxTXzPbsf6Wq-4mCK3n1vmiYIcOP1DILxfqNaEbhtZLapFHhPhPVHRiUxv_uQI1366gPXT9dih7xQcIGhAJdgkMwiH_1RxQtskxG0f2yWT2uYfzMcQeZdPiUp6i2yv1DieIowNL12xO8mT5LbA_eoBvz7EwrD2EG_mWVcnOt9cn_ouyvNzSduuF9gNcBNsdiNuu_oEQZuTedvJiOlmHjPx4-r6LWItKz2on1_r6Ah8I6NWlCiu_uxqkCCEp_T62EtHvnFjLo-pXLaZJiMgltYTXQwmm0uM3zICTK8WTfa_NEUtyItufcqnywNpK6-sTiN9EtTM0-vhhCJH8s6KO_HIrUJ1lLPNcKdKy5NdGeY_EfjOS5jPd9mV3wdss9asYHBDM3X6vnJRy-gLQka9kHLMUrw0_HvmBi_C1vPXy3UGTS2tDowxRUMZxAI4lhsf6DDfIBvjDQq2-MCd_E3Mv5UtFzWmZX9gAptN1uLn83rGK7p-To1O0_fIK3aS3Tzd6UK5VsWYz4vTEHcbyV-tiC1lBbqKSzXhjEN5i14kKT9SFDfL3Anf9SE9qcrE8kekWtr10wP99vuGl0YAJ4NA_FObf0f-R46G7biK4XbX6TWx11tu19jtPLfYs8n-kXZ9Tsbfkai6izU9z7CmpZa8grvHIkrT1j6PodgJdA5LifpeXZiTVSVbrGYq6p_j9TEr0ybJwL8wVAwFnxAEj-verxtBQq2Z3aTB4x7DiUP4DMnpE_TJqJHDaNNopEUtZ7kT_sCtDLSqEYPpz2aay13w9ykamP_3e4zptUWLG16Xk-DJyApRTLmvvMHEI61xKsN_Lk2LHf3d94CcWK785knmoBOfaq2IoCg53pcSZmbHCFM6h-_YEht6lghkWAc0nCWcpNvAFHvwDB6_-_4BHMN_-0R0XMC-ptAQEfmuleiRh-4DbNPpKXbkNStCsWCohH9veA3CBMo4mvje40v5Hh'}]), native_finish_reason='completed')], created=1768675409, model='openai/gpt-5.2-codex', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=73, prompt_tokens=135, total_tokens=208, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=40, rejected_prediction_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0), cost=0.00125825, is_byok=False, cost_details={'upstream_inference_cost': 0.00125825, 'upstream_inference_prompt_cost': 0.00023625, 'upstream_inference_completions_cost': 0.001022}), provider='OpenAI')\n"
     ]
    }
   ],
   "source": [
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23cbfcc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not ChatCompletion",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m response2_obj = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(response2_obj[\u001b[33m'\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\conda_envs\\langggraphh\\Lib\\json\\__init__.py:345\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    346\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    347\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    350\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    351\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[31mTypeError\u001b[39m: the JSON object must be str, bytes or bytearray, not ChatCompletion"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response2_obj = json.loads(response2)\n",
    "print(response2_obj['choices'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ac4e87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"gen-1768675229-8yRln5N5TR1spTh7WoPm\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"Yes\\u2014**there are 3 r\\u2019s** in \\u201cstrawberry.\\u201d\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"annotations\": null,\n",
      "        \"audio\": null,\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null,\n",
      "        \"reasoning\": \"**Confirming count of \\\"r\\\"s**\\n\\nThe word \\\"strawberry\\\" contains three \\\"r\\\" letters at positions 3, 8, and 9; this count is certain and accurate.\",\n",
      "        \"reasoning_details\": [\n",
      "          {\n",
      "            \"format\": \"openai-responses-v1\",\n",
      "            \"index\": 0,\n",
      "            \"type\": \"reasoning.summary\",\n",
      "            \"summary\": \"**Confirming count of \\\"r\\\"s**\\n\\nThe word \\\"strawberry\\\" contains three \\\"r\\\" letters at positions 3, 8, and 9; this count is certain and accurate.\"\n",
      "          },\n",
      "          {\n",
      "            \"id\": \"rs_008364eeb9becb6c01696bd79db8788190b05530f8f2f1a271\",\n",
      "            \"format\": \"openai-responses-v1\",\n",
      "            \"index\": 0,\n",
      "            \"type\": \"reasoning.encrypted\",\n",
      "            \"data\": \"gAAAAABpa9efNJfk4q1eQCt5Wvf-N-nwak9lUy_MIjd_G_AP5x6S2aG4-RNmh-keTQmkHEMCSA-fLzRq0MNm7_I84PkanWURa_iU5_sKnsuAxzwkPxmUFaguBVnrBujlux0-TaPaDPsGTcr7ZB4cA4vFWXahCx_DabRoKYE-gK-vJ4igxs_j6sNw6QuiBaUW3aOOPDHWmqhpmmtFzGM7jR7xKKJkFlgKMhWVfwKDslPAg7qLnVvAFjI1aegGt1ibO0zNWeowEq-VdE48OSwvaI-8D2aGVt35KlJVzMhu3QmsrCyGEL6gWoYPUHzVhrXD77eKF_O9GaPcwrS5txofwb3znHokzxk4ee_f4jkISJSAIj0SlHgH2k7ivs84UYCO_R0ArSArOmIAVpI1myjLtEVZblVIV9BpdxobXxBCrtbIgHo2zlAhju4lSqHV3gjK9XqPgeidpjhfo5c-lxuSIBZBmdI0gHFeeR3j_hcWpEi6gnddSQf2WBBdt543AWDKqcP3Gx4wufsMP4eGosZAs0Rs_E5qG8c4f-iNhCfwQ9Vyh9N8C3MxaXCKZ8Sn16v3QcImwfsyIXyHbQ01qb2xt3MqdcRXCOjQfKgvXr5B6IbQ6d3AS3F9Qd4dTHVUICuUA3rPD4dwfkhxTFY7UPlR3irKT5wKIw351laL--ut6sNpwHz_3kuu1_Rofts0cyiywvLEZ1-RfzbY-P0M0mB09_ZtLSP-xB7SkrUi7ozaDi5UfkOgFeOv7uf2clDrUDe4ZYB0aCvsGbZE8P82KvXPz_O29Dm02J9avIUsYh43rZeC1tCDE4qeskElDMMu9zstNtFxUXtRD-j1qm3dg43GoS7i0QoBXNyEAPAiFwBKB3cxuNgOk-ARDej0zosctSpMCxfNT9ZWvtaze-_PAuuLrWZrZ3CFuh6jqiAJ8BJCLI5zjRlZQwogo54NkPXhxZ1jTVQ441VvIRrsCoACfk4v3TN7onQgxnL_d2qh1UQofN4QNa2elD3t9L8AGEIO51v20dbjoVbqnGVmgkcQtaV6yOuEhyCKjR0PGicqy6pYZQJtuz0kvZZB0m0M7WFzOuIzUadKr12TxIY4zkeMvA_7qg67WyMO5Sbgpuse-kaqdsz6T9OEK5ChrgPYGzisadG_FNN1Fx9KLFPFcHwOL8rI96yWYclp57ZvRg==\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"native_finish_reason\": \"completed\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1768675229,\n",
      "  \"model\": \"openai/gpt-5.2-codex\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"service_tier\": null,\n",
      "  \"system_fingerprint\": null,\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 86,\n",
      "    \"prompt_tokens\": 53,\n",
      "    \"total_tokens\": 139,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": null,\n",
      "      \"audio_tokens\": null,\n",
      "      \"reasoning_tokens\": 64,\n",
      "      \"rejected_prediction_tokens\": null,\n",
      "      \"image_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": null,\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"cost\": 0.00129675,\n",
      "    \"is_byok\": false,\n",
      "    \"cost_details\": {\n",
      "      \"upstream_inference_cost\": 0.00129675,\n",
      "      \"upstream_inference_prompt_cost\": 9.275e-05,\n",
      "      \"upstream_inference_completions_cost\": 0.001204\n",
      "    }\n",
      "  },\n",
      "  \"provider\": \"OpenAI\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Pretty print the response2_obj in a formatted way\n",
    "\n",
    "print(json.dumps(response2_obj, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102190d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langggraphh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
